"""
LLM Service - Simplified with OpenRouter

OpenRouter provides a single API for all major LLMs:
- OpenAI (GPT-4o, GPT-5)
- Anthropic (Claude Opus 4, Claude Sonnet 4)
- Google (Gemini)
- Meta (Llama)
- And many more...

Uses OpenAI-compatible API format.
"""

from typing import Optional
import asyncio

from app.config import get_settings


# Model ID to OpenRouter model name mapping
MODEL_MAP = {
    # OpenAI
    "gpt-5": "openai/gpt-4o",  # Using gpt-4o as gpt-5 may not exist
    "gpt-4o": "openai/gpt-4o",
    
    # Anthropic
    "claude-opus-4": "anthropic/claude-sonnet-4",  # Using latest available
    "claude-sonnet-4": "anthropic/claude-sonnet-4",
    
    # Google
    "gemini-2.5-pro": "google/gemini-2.0-flash-001",
    "gemini-2.5-flash": "google/gemini-2.0-flash-001",
    
    # DeepSeek
    "deepseek-r1": "deepseek/deepseek-r1",
    "deepseek-v3": "deepseek/deepseek-chat",
    
    # Meta
    "llama-4-405b": "meta-llama/llama-3.3-70b-instruct",  # Using available Llama
    
    # Mistral
    "mistral-large": "mistralai/mistral-large-2411",
    
    # Alibaba
    "qwen-max": "qwen/qwen-2.5-72b-instruct",
    
    # xAI
    "grok-3": "x-ai/grok-2-1212",  # Using available Grok
}


class OpenRouterProvider:
    """OpenRouter API provider - supports all major LLMs."""
    
    def __init__(self):
        self.settings = get_settings()
        self.base_url = "https://openrouter.ai/api/v1"
    
    async def generate(
        self, 
        model_id: str,
        prompt: str, 
        system_prompt: Optional[str] = None
    ) -> str:
        from openai import AsyncOpenAI
        
        client = AsyncOpenAI(
            api_key=self.settings.openrouter_api_key,
            base_url=self.base_url,
        )
        
        # Get OpenRouter model name
        model_name = MODEL_MAP.get(model_id, "openai/gpt-4o")
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = await client.chat.completions.create(
            model=model_name,
            messages=messages,
            max_tokens=2000,
            temperature=0.7,
            extra_headers={
                "HTTP-Referer": "https://chemistry-arena.app",
                "X-Title": "Chemistry Arena",
            }
        )
        
        return response.choices[0].message.content or ""


class MockProvider:
    """Mock provider for testing without API calls."""
    
    async def generate(
        self, 
        model_id: str,
        prompt: str, 
        system_prompt: Optional[str] = None
    ) -> str:
        # Simulate API latency
        await asyncio.sleep(0.5)
        
        return f"""Based on my analysis of this chemistry problem, here are the key points:

**Analysis:**
The question involves fundamental concepts in chemistry that require careful consideration of:
1. Reaction mechanisms and kinetics
2. Thermodynamic factors (ΔG, ΔH, ΔS)
3. Structural and electronic effects

**Key Considerations:**
- The reaction pathway follows standard mechanistic principles
- Electronic effects play a significant role in determining selectivity
- Steric factors influence the preferred conformation

**Conclusion:**
The expected product would be formed via the most thermodynamically favorable pathway, with careful attention to stereochemical outcomes.

[Response generated by {model_id} for testing purposes]"""


class LLMService:
    """
    Main LLM service - routes all requests through OpenRouter.
    Falls back to mock responses if no API key is configured.
    """
    
    # Default system prompt for chemistry questions (hidden from frontend)
    CHEMISTRY_SYSTEM_PROMPT = """You are an expert chemistry assistant specializing in drug discovery, ADMET properties, and molecular design.

IMPORTANT FORMATTING RULES:
- Limit your response to exactly 3 bullet points
- Each bullet should be concise but informative (1-2 sentences max)
- Use proper chemical notation (SMILES, IUPAC names) when relevant
- Be precise and scientifically accurate

Focus on the most important aspects of the question."""
    
    def __init__(self, use_mock: bool = False):
        """
        Initialize LLM service.
        
        Args:
            use_mock: If True, use mock provider for all requests (for testing)
        """
        self.use_mock = use_mock
        self.settings = get_settings()
        self.openrouter = OpenRouterProvider()
        self.mock = MockProvider()
    
    def _has_api_key(self) -> bool:
        """Check if OpenRouter API key is configured."""
        return bool(self.settings.openrouter_api_key)
    
    async def generate_response(
        self,
        model_id: str,
        provider_name: str,  # Kept for compatibility but not used
        prompt: str,
        system_prompt: Optional[str] = None
    ) -> str:
        """
        Generate a response from the specified model via OpenRouter.
        
        Args:
            model_id: The model identifier
            provider_name: The provider name (ignored - using OpenRouter)
            prompt: The user prompt
            system_prompt: Optional system prompt (uses default chemistry prompt if None)
        
        Returns:
            The generated response text
        """
        # Use chemistry system prompt by default
        if system_prompt is None:
            system_prompt = self.CHEMISTRY_SYSTEM_PROMPT
        
        # Use mock if no API key or explicitly requested
        if self.use_mock or not self._has_api_key():
            return await self.mock.generate(model_id, prompt, system_prompt)
        
        try:
            response = await self.openrouter.generate(model_id, prompt, system_prompt)
            return response
        except Exception as e:
            # Fall back to mock on error
            print(f"Error calling OpenRouter API for {model_id}: {e}")
            return await self.mock.generate(model_id, prompt, system_prompt)
    
    async def generate_battle_responses(
        self,
        model_a_id: str,
        model_a_provider: str,
        model_b_id: str,
        model_b_provider: str,
        prompt: str
    ) -> tuple[str, str]:
        """
        Generate responses from two models in parallel.
        
        Returns:
            Tuple of (response_a, response_b)
        """
        # Run both requests in parallel
        response_a, response_b = await asyncio.gather(
            self.generate_response(model_a_id, model_a_provider, prompt),
            self.generate_response(model_b_id, model_b_provider, prompt),
        )
        
        return response_a, response_b
